"""
experiment code
Author: Jay C. Rothenberger (jay.c.rothenberger@ou.edu)
Contributor: Vincent A. Ferrera (vaflyers@gmail.com)
"""
import os
import pickle
import gc
# model-related code I have written (supplied locally)
from cnn_network import *
from data_structures import ModelData
from data_generator import augment_with_neighbors, to_dataset, to_flow


def generate_fname(args):
    """
    Generate the base file name for output files/directories.

    The approach is to encode the key experimental parameters in the file name.  This
    way, they are unique and easy to identify after the fact.

    :param args: from argParse
    :params_str: String generated by the JobIterator
    :return: a string (file name prefix)
    """
    # network parameters
    hidden_str = '_'.join([str(i) for i in args.hidden])
    filters_str = '_'.join([str(i) for i in args.filters])
    kernels_str = '_'.join([str(i) for i in args.kernels])

    # Label
    if args.label is None:
        label_str = ""
    else:
        label_str = "%s_" % args.label

    # Experiment type
    if args.exp_type is None:
        experiment_type_str = ""
    else:
        experiment_type_str = "%s_" % args.exp_type

    # experiment index
    num_str = str(args.exp)

    # learning rate
    lrate_str = "LR_%0.6f_" % args.lrate

    return "%s/%s_%s_filt_%s_ker_%s_hidden_%s_l1_%s_l2_%s_drop_%s_frac_%s_lrate_%s_%s" % (
        args.results_path,
        experiment_type_str,
        num_str,
        filters_str,
        kernels_str,
        hidden_str,
        str(args.l1),
        str(args.l2),
        str(args.dropout),
        str(args.train_fraction),
        lrate_str,
        str(time()).replace('.', '')[-6:])


def execute_exp(args, model, train_dset, val_dset, network_fn, network_params, train_iteration=0,
                train_steps=None, val_steps=None, callbacks=None, evaluate_on=None):
    """
    Perform the training and evaluation for a single model

    :param args: Argparse arguments object
    :param model: keras model
    :param train_dset: training dataset instance
    :param val_dset: validation dataset instance
    :param train_iteration: training iteration
    :param train_steps: number of training steps to perform per epoch
    :param val_steps: number of validation steps to perform per epoch
    :param callbacks: callbacks for model.fit
    :param evaluate_on: a dictionary of objects on which to call model.evaluate (take care they are not infinite)
    :return: trained keras model encoded as a ModelData instance
    """

    print(args.exp)

    fbase = generate_fname(args)

    print(fbase)
    print(model.summary())
    tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True,
                              to_file=os.curdir + f'/../visualizations/models/model_{str(time())[:6]}.png')
    # Perform the experiment?
    if args.nogo:
        # No!
        print("NO GO")
        return

    # Learn
    #  steps_per_epoch: how many batches from the training set do we use for training in one epoch?
    #  validation_steps=None
    #  means that ALL validation samples will be used (of the selected subset)
    history = model.fit(train_dset,
                        epochs=args.epochs,
                        verbose=True,
                        validation_data=val_dset,
                        callbacks=callbacks,
                        steps_per_epoch=train_steps,
                        validation_steps=val_steps,
                        shuffle=False)

    evaluate_on = dict() if evaluate_on is None else evaluate_on

    evaluations = {k: model.evaluate(evaluate_on[k]) for k in evaluate_on}

    # populate results data structure
    model_data = ModelData(weights=model.get_weights(),
                           network_params=network_params,
                           network_fn=network_fn,
                           evaluations=evaluations,
                           classes=network_params['n_classes'],
                           history=history.history,
                           train_fraction=args.train_fraction,
                           train_iteration=train_iteration,
                           args=args)
    return model_data


def start_training(args, model, train_dset, val_dset, network_fn, network_params,
                   evaluate_on=None, train_steps=None, val_steps=None):
    """
    TODO
    """
    # Override arguments if we are using exp_index

    train_steps = train_steps if train_steps is not None else 100
    val_steps = val_steps if val_steps is not None else 100

    print(train_steps, val_steps)

    evaluate_on = dict() if evaluate_on is None else evaluate_on

    callbacks = [tf.keras.callbacks.EarlyStopping(patience=args.patience,
                                                  restore_best_weights=True,
                                                  min_delta=args.min_delta)]

    return execute_exp(args, model, train_dset, val_dset, network_fn, network_params,
                       0, train_steps, val_steps, callbacks=callbacks, evaluate_on=evaluate_on)


def self_train(args, network_fn, network_params, train_df, val_df, unlabeled_df, image_size=(256, 256),
               dataset_fn=None, evaluate_on=None):
    """
    TODO

    """
    print('args', args)
    import distances
    dist_func_selector = {
        'euclidean': distances.euclidean,
        'euclidean_with_confidence': distances.euclidean_with_confidence,
        'confidence': distances.confidence
    }

    default_image_gen = tf.keras.preprocessing.image.ImageDataGenerator()

    model = None
    labeled = train_df
    for train_iteration in range(args.train_iterations):
        if not train_iteration or args.retrain_fully:
            model = network_fn(**network_params)

        model_data = start_training(args,
                                    model,
                                    dataset_fn(train_df, train=True),
                                    dataset_fn(val_df),
                                    network_fn=network_fn,
                                    network_params=network_params,
                                    train_steps=args.steps_per_epoch,
                                    val_steps=len(val_df) // args.batch,
                                    evaluate_on=evaluate_on
                                    )

        with open('%s/model_%s_%s_%s' % (args.results_path,
                                         str(args.train_fraction),
                                         str(train_iteration),
                                         str(time())), 'wb') as fp:
            pickle.dump(model_data, fp)

        distance_function = dist_func_selector.get(args.distance_function)

        if distance_function is None:
            raise ValueError('unrecognized experiment type')

        if unlabeled_df is not None and (len(unlabeled_df) - args.augment_batch) > 0:
            print(len(unlabeled_df))
            tf.keras.backend.clear_session()
            gc.collect()

            # replace the generators and dataframes with the updated ones from augment_with_neighbors
            train_df, unlabeled_df, labeled = augment_with_neighbors(args,
                                                                     model,
                                                                     image_size,
                                                                     distance_fn=distance_function,
                                                                     image_gen=default_image_gen,
                                                                     pseudolabeled_data=train_df,
                                                                     unlabeled_data=unlabeled_df,
                                                                     labeled_data=labeled,
                                                                     sample=args.sample)
        else:
            print('not enough data for an additional iteration of training!')
            print(f'stopped after {train_iteration} iterations!')
            break
        print('retraining: ', train_iteration)
