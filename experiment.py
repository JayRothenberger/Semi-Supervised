"""
experiment code
Author: Jay C. Rothenberger (jay.c.rothenberger@ou.edu)
Contributor: Vincent A. Ferrera (vaflyers@gmail.com)
"""
import os
import pickle
import gc
# model-related code I have written (supplied locally)
from cnn_network import *
from data_structures import ModelData
from data_generator import augment_with_neighbors, to_dataset, cifar10_dset, cifar100_dset, blended_dset, bc_plus,\
    generalized_bc_plus, load_unlabeled, get_cv_rotation, get_dataframes_self_train, mixup_dset
from transforms import custom_rand_augment_object
from make_figure import explore_image_dataset


def generate_fname(args):
    """
    Generate the base file name for output files/directories.

    The approach is to encode the key experimental parameters in the file name.  This
    way, they are unique and easy to identify after the fact.

    :param args: from argParse
    :params_str: String generated by the JobIterator
    :return: a string (file name prefix)
    """
    # network parameters
    hidden_str = '_'.join([str(i) for i in args.hidden])
    filters_str = '_'.join([str(i) for i in args.filters])
    kernels_str = '_'.join([str(i) for i in args.kernels])

    # Label
    if args.label is None:
        label_str = ""
    else:
        label_str = "%s_" % args.label

    # Experiment type
    if args.exp_type is None:
        experiment_type_str = ""
    else:
        experiment_type_str = "%s_" % args.exp_type

    # experiment index
    num_str = str(args.exp)

    # learning rate
    lrate_str = "LR_%0.6f_" % args.lrate

    return "%s/%s_%s_filt_%s_ker_%s_hidden_%s_l1_%s_l2_%s_drop_%s_frac_%s_lrate_%s_%s" % (
        args.results_path,
        experiment_type_str,
        num_str,
        filters_str,
        kernels_str,
        hidden_str,
        str(args.l1),
        str(args.l2),
        str(args.dropout),
        str(args.train_fraction),
        lrate_str,
        str(time()).replace('.', '')[-6:])


def execute_exp(args, model, train_dset, val_dset, network_fn, network_params, train_iteration=0,
                train_steps=None, val_steps=None, callbacks=None, evaluate_on=None):
    """
    Perform the training and evaluation for a single model

    :param args: Argparse arguments object
    :param model: keras model
    :param train_dset: training dataset instance
    :param val_dset: validation dataset instance
    :param train_iteration: training iteration
    :param train_steps: number of training steps to perform per epoch
    :param val_steps: number of validation steps to perform per epoch
    :param callbacks: callbacks for model.fit
    :param evaluate_on: a dictionary of objects on which to call model.evaluate (take care they are not infinite)
    :return: trained keras model encoded as a ModelData instance
    """

    print(args.exp)

    fbase = generate_fname(args)

    print(fbase)
    print(model.summary())
    tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True,
                              to_file=os.curdir + f'/../visualizations/models/model_{str(time())[:6]}.png')
    # Perform the experiment?
    if args.nogo:
        # No!
        print("NO GO")
        return

    # Learn
    #  steps_per_epoch: how many batches from the training set do we use for training in one epoch?
    #  validation_steps=None
    #  means that ALL validation samples will be used (of the selected subset)
    history = model.fit(train_dset,
                        epochs=args.epochs,
                        verbose=True,
                        validation_data=val_dset,
                        callbacks=callbacks,
                        steps_per_epoch=train_steps,
                        validation_steps=val_steps,
                        shuffle=False)

    evaluate_on = dict() if evaluate_on is None else evaluate_on

    evaluations = {k: model.evaluate(evaluate_on[k]) for k in evaluate_on}

    # populate results data structure
    model_data = ModelData(weights=model.get_weights(),
                           network_params=network_params,
                           network_fn=network_fn,
                           evaluations=evaluations,
                           classes=network_params['n_classes'],
                           history=history.history,
                           train_fraction=args.train_fraction,
                           train_iteration=train_iteration,
                           args=args)
    return model_data


def start_training(args, model, train_dset, val_dset, network_fn, network_params,
                   evaluate_on=None, train_steps=None, val_steps=None):
    """
    train a keras model on a dataset and evaluate it on other datasets then return the ModelData instance

    :param args: Argparse args object from the CLA
    :param model: keras model
    :param train_dset: tf.data.Dataset for training
    :param val_dset: tf.data.Dataset for evaluation
    :param network_fn: function that was used to build the keras model
    :param network_params: parameters passed to the function used to build the keras model
    :param evaluate_on: a dictionary of finite objects passable to model.evaluate
    :param train_steps: number of steps per epoch
    :param val_steps: number of validations steps per epoch
    :return: a ModelData instance
    """
    # Override arguments if we are using exp_index

    train_steps = train_steps if train_steps is not None else 100
    val_steps = val_steps if val_steps is not None else 100

    print(train_steps, val_steps)

    evaluate_on = dict() if evaluate_on is None else evaluate_on

    callbacks = [tf.keras.callbacks.EarlyStopping(patience=args.patience,
                                                  restore_best_weights=True,
                                                  min_delta=args.min_delta)]

    return execute_exp(args, model, train_dset, val_dset, network_fn, network_params,
                       0, train_steps, val_steps, callbacks=callbacks, evaluate_on=evaluate_on)


def self_train(args, network_fn, network_params, train_df, val_df, unlabeled_df, image_size=(256, 256),
               dataset_fn=None, evaluate_on=None):
    """
    perform a self-training experiment

    :param args: CLA
    :param network_fn: function used to build the network
    :param network_params: arguments to be passed to that function
    :param train_df: training dataframe - image paths of training images
    :param val_df: validation dataframe - image paths
    :param unlabeled_df: unlabeled dataframe - image paths (does not try to infer a class directory)
    :param image_size: (W, H)
    :param dataset_fn: function used to construct the tf.data.Dataset the model will train on
    :param evaluate_on: dictionary of items to pass to model.evaluate in the evaluation steps

    """
    print('args', args)
    import distances
    dist_func_selector = {
        'euclidean': distances.euclidean,
        'euclidean_with_confidence': distances.euclidean_with_confidence,
        'confidence': distances.confidence
    }

    model = None
    labeled = train_df
    for train_iteration in range(args.train_iterations):
        if not train_iteration or args.retrain_fully:
            if args.distributed:
                # create the scope
                strategy = tf.distribute.MirroredStrategy()
                with strategy.scope():
                    # build the model (in the scope)
                    model = network_fn(**network_params)
            else:
                model = network_fn(**network_params)

        model_data = start_training(args,
                                    model,
                                    dataset_fn(train_df, train=True, cache=args.cache),
                                    dataset_fn(val_df, cache=args.cache),
                                    network_fn=network_fn,
                                    network_params=network_params,
                                    train_steps=args.steps_per_epoch,
                                    val_steps=len(val_df) // args.batch,
                                    evaluate_on=evaluate_on
                                    )

        # save the model
        try:
            with open(f'{os.curdir}/../results/{train_iteration}_{time()}', 'wb') as fp:
                pickle.dump(model_data, fp)
        except Exception as e:
            print(e)
            with open(f'./{generate_fname(args)}_{train_iteration}', 'wb') as fp:
                pickle.dump(model_data, fp)

        distance_function = dist_func_selector.get(args.distance_function)

        if distance_function is None:
            raise ValueError('unrecognized experiment type')

        if unlabeled_df is not None and (len(unlabeled_df) - args.augment_batch) > 0:
            print(len(unlabeled_df))
            tf.keras.backend.clear_session()
            gc.collect()

            # replace the generators and dataframes with the updated ones from augment_with_neighbors
            train_df, unlabeled_df, labeled = augment_with_neighbors(args,
                                                                     model,
                                                                     image_size,
                                                                     distance_fn=distance_function,
                                                                     pseudolabeled_data=train_df,
                                                                     unlabeled_data=unlabeled_df,
                                                                     labeled_data=labeled,
                                                                     sample=args.sample)
        else:
            print('not enough data for an additional iteration of training!')
            print(f'stopped after {train_iteration} iterations!')
            break
        print('retraining: ', train_iteration)


def cifar(args, da_fn, da_args, n_classes=10):
    # TODO: cross-validation
    image_size, n_classes = (32, 32), n_classes

    network_params = {'learning_rate': args.lrate,
                      'conv_filters': args.filters,
                      'conv_size': args.kernels,
                      'attention_heads': args.hidden,
                      'image_size': (image_size[0], image_size[1], 3),
                      'n_classes': n_classes,
                      'l1': args.l1,
                      'l2': args.l2,
                      'dropout': args.dropout,
                      'loss': 'categorical_crossentropy',
                      'pad': 4,
                      'overlap': 4}

    print('hidden', args.hidden)
    network_fn = build_transformer_4

    switch = {
        10: cifar10_dset,
        100: cifar100_dset
    }

    train_dset, val_dset, test_dset = switch.get(n_classes)(batch_size=args.batch, cache=args.cache)
    # define our randAugment object
    rand_aug = custom_rand_augment_object(args.rand_M, args.rand_N, True)
    # data augmentation strategy
    if args.randAugment:
        train_dset = train_dset.map(lambda x, y: (tf.py_function(rand_aug, [x], [tf.float32])[0], y),
                                    num_parallel_calls=tf.data.AUTOTUNE, )
    # perform MSDA
    train_dset = da_fn(train_dset, **da_args)

    # peek at the dataset instead of training
    if args.peek:
        explore_image_dataset(train_dset, 8)
        exit(-1)
    if args.distributed:
        # create the scope
        strategy = tf.distribute.MirroredStrategy()
        with strategy.scope():
            # build the model (in the scope)
            model = network_fn(**network_params)
    else:
        model = network_fn(**network_params)
    # train the model
    model_data = start_training(args, model, train_dset, val_dset, network_fn, network_params,
                                train_steps=args.steps_per_epoch, val_steps=10000 // args.batch)
    # save the model
    try:
        with open(f'{os.curdir}/../results/{generate_fname(args)}', 'wb') as fp:
            pickle.dump(model_data, fp)
    except Exception as e:
        print(e)
        with open(f'./{generate_fname(args)}', 'wb') as fp:
            pickle.dump(model_data, fp)


def cifar10(args, da_fn, da_args):
    cifar(args, da_fn, da_args, 10)


def cifar100(args, da_fn, da_args):
    cifar(args, da_fn, da_args, 100)


def DOT_CV_self_train(args, da_fn, da_args):
    # TODO: cross-validation, peeking
    if args.peek:
        raise NotImplementedError('peeking is not implemented for this experiment type')

    image_size, n_classes = (256, 256), 3

    paths = ['bronx_allsites/wet', 'bronx_allsites/dry', 'bronx_allsites/snow',
             'ontario_allsites/wet', 'ontario_allsites/dry', 'ontario_allsites/snow',
             'rochester_allsites/wet', 'rochester_allsites/dry', 'rochester_allsites/snow']

    unlabeled_paths = [os.curdir + '/../unlabeled/']

    unlabeled_df = load_unlabeled(unlabeled_paths)

    print('getting dsets...')
    if args.cross_validate:
        print(f'({args.cv_k}-fold) cross-validation rotation: {args.cv_rotation}')
        train_df, withheld_df, val_df, test_df = get_cv_rotation(
            [os.curdir + '/../data/' + path for path in paths],
            train_fraction=args.train_fraction, k=args.cv_k, rotation=args.cv_rotation)
    else:
        train_df, withheld_df, val_df, test_df, ig = get_dataframes_self_train(
            [os.curdir + '/../data/' + path for path in paths],
            train_fraction=args.train_fraction)

    network_params = {'learning_rate': args.lrate,
                      'conv_filters': args.filters,
                      'conv_size': args.kernels,
                      'attention_heads': args.hidden,
                      'image_size': (image_size[0], image_size[1], 3),
                      'n_classes': n_classes,
                      'l1': args.l1,
                      'l2': args.l2,
                      'dropout': args.dropout,
                      'loss': 'categorical_crossentropy'}

    network_fn = build_patchwise_vision_transformer

    def dataset_fn(df, train=False, cache=True):
        ds = to_dataset(df, shuffle=True, image_size=image_size, batch_size=args.batch, seed=42,
                        class_mode='categorical', cache=cache)
        if train:
            # define our randAugment object
            rand_aug = custom_rand_augment_object(args.rand_M, args.rand_N, True)
            # data augmentation strategy
            if args.randAugment:
                ds = ds.map(lambda x, y: (tf.py_function(rand_aug, [x], [tf.float32])[0], y),
                            num_parallel_calls=tf.data.AUTOTUNE, )

            # perform MSDA
            ds = da_fn(ds, **da_args)

        return ds

    self_train(args, network_fn, network_params, train_df, val_df, unlabeled_df,
               dataset_fn=dataset_fn,
               evaluate_on=None)


def DOT_CV(args, da_fn, da_args):
    image_size, n_classes = (256, 256), 3

    paths = ['bronx_allsites/wet', 'bronx_allsites/dry', 'bronx_allsites/snow',
             'ontario_allsites/wet', 'ontario_allsites/dry', 'ontario_allsites/snow',
             'rochester_allsites/wet', 'rochester_allsites/dry', 'rochester_allsites/snow']

    print('getting dsets...')
    if args.cross_validate:
        print(f'({args.cv_k}-fold) cross-validation rotation: {args.cv_rotation}')
        train_df, withheld_df, val_df, test_df = get_cv_rotation(
            [os.curdir + '/../data/' + path for path in paths],
            train_fraction=args.train_fraction, k=args.cv_k, rotation=args.cv_rotation)
    else:
        train_df, withheld_df, val_df, test_df, ig = get_dataframes_self_train(
            [os.curdir + '/../data/' + path for path in paths],
            train_fraction=args.train_fraction)

    network_params = {'learning_rate': args.lrate,
                      'conv_filters': args.filters,
                      'conv_size': args.kernels,
                      'attention_heads': args.hidden,
                      'image_size': (image_size[0], image_size[1], 3),
                      'n_classes': n_classes,
                      'l1': args.l1,
                      'l2': args.l2,
                      'dropout': args.dropout,
                      'loss': 'categorical_crossentropy',
                      'pad': 24,
                      'overlap': 8}

    network_fn = build_MobileNetV3Small

    val_dset = to_dataset(val_df, shuffle=True, batch_size=args.batch, seed=42, prefetch=8,
                          class_mode='categorical', center=True, cache=args.cache)
    train_dset = to_dataset(train_df, shuffle=True, batch_size=args.batch, seed=42, prefetch=8,
                            class_mode='categorical', center=True, cache=args.cache)

    # define our randAugment object
    rand_aug = custom_rand_augment_object(args.rand_M, args.rand_N, True)
    # data augmentation strategy
    if args.randAugment:
        train_dset = train_dset.map(lambda x, y: (tf.py_function(rand_aug, [x], [tf.float32])[0], y),
                                    num_parallel_calls=tf.data.AUTOTUNE, )

    # perform MSDA
    train_dset = da_fn(train_dset, **da_args)
    # peek at the dataset instead of training
    if args.peek:
        explore_image_dataset(train_dset, 8)
        exit(-1)
    if args.distributed:
        # create the scope
        strategy = tf.distribute.MirroredStrategy()
        with strategy.scope():
            # build the model (in the scope)
            model = network_fn(**network_params)
    else:
        model = network_fn(**network_params)
    # train the model
    model_data = start_training(args, model, train_dset, val_dset, network_fn, network_params,
                                train_steps=args.steps_per_epoch, val_steps=len(val_df) // args.batch)
    # save the model
    try:
        with open(f'{os.curdir}/../results/{generate_fname(args)}', 'wb') as fp:
            pickle.dump(model_data, fp)
    except Exception as e:
        print(e)
        with open(f'./{generate_fname(args)}', 'wb') as fp:
            pickle.dump(model_data, fp)